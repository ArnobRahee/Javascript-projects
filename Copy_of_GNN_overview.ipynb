{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArnobRahee/Javascript-projects/blob/main/Copy_of_GNN_overview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tK2w51z3DNk"
      },
      "source": [
        "# Tutorial 7: Graph Neural Networks\n",
        "\n",
        "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=green)\n",
        "\n",
        "**Filled notebook:**\n",
        "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/GNN_overview.ipynb)\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/GNN_overview.ipynb)  \n",
        "**Pre-trained models:**\n",
        "[![View files on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/saved_models/tree/main/tutorial7)\n",
        "[![GoogleDrive](https://img.shields.io/static/v1.svg?logo=google-drive&logoColor=yellow&label=GDrive&message=Download&color=yellow)](https://drive.google.com/drive/folders/1DOTV_oYt5boa-MElbc2izat4VMSc1gob?usp=sharing)   \n",
        "**Recordings:**\n",
        "[![YouTube - Part 1](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%201&color=red)](https://youtu.be/fK7d56Ly9q8)\n",
        "[![YouTube - Part 2](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%202&color=red)](https://youtu.be/ZCNSUWe4a_Q)   \n",
        "**JAX+Flax version:**\n",
        "[![View on RTD](https://img.shields.io/static/v1.svg?logo=readthedocs&label=RTD&message=View%20On%20RTD&color=8CA1AF)](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial7/GNN_overview.html)   \n",
        "**Author:** Phillip Lippe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdL0_bLw3DNr"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "\n",
        "**Note:** Interested in JAX? Check out our [JAX+Flax version](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial7/GNN_overview.html) of this tutorial!\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n20ARdNS3DNs"
      },
      "source": [
        "In this tutorial, we will discuss the application of neural networks on graphs. Graph Neural Networks (GNNs) have recently gained increasing popularity in both applications and research, including domains such as social networks, knowledge graphs, recommender systems, and bioinformatics. While the theory and math behind GNNs might first seem complicated, the implementation of those models is quite simple and helps in understanding the methodology. Therefore, we will discuss the implementation of basic network layers of a GNN, namely graph convolutions, and attention layers. Finally, we will apply a GNN on a node-level, edge-level, and graph-level tasks.\n",
        "\n",
        "Below, we will start by importing our standard libraries. We will use PyTorch Lightning as already done in Tutorial 5 and 6."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/deepmind/jraph.git\n",
        "!pip install flax\n",
        "!pip install dm-haiku"
      ],
      "metadata": {
        "id": "AkNdUVrF5WVq",
        "outputId": "216919e2-6949-498e-c39e-b2587abbde98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/deepmind/jraph.git\n",
            "  Cloning https://github.com/deepmind/jraph.git to /tmp/pip-req-build-6l4yerqp\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/deepmind/jraph.git /tmp/pip-req-build-6l4yerqp\n",
            "  Resolved https://github.com/deepmind/jraph.git to commit 51f5990104f7374492f8f3ea1cbc47feb411c69c\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.10/dist-packages (from jraph==0.0.6.dev0) (0.4.23)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from jraph==0.0.6.dev0) (0.4.23+cuda12.cudnn89)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from jraph==0.0.6.dev0) (1.25.2)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->jraph==0.0.6.dev0) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->jraph==0.0.6.dev0) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->jraph==0.0.6.dev0) (1.11.4)\n",
            "Building wheels for collected packages: jraph\n",
            "  Building wheel for jraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jraph: filename=jraph-0.0.6.dev0-py3-none-any.whl size=91245 sha256=7dbe43ba0adb4a468135023c1c8ae8f6debb496c348553a118160a44c1194e55\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1fkjxsdu/wheels/76/61/34/8fb9aa4dac00d471de4a5f7157614181de683c445fc2d640db\n",
            "Successfully built jraph\n",
            "Installing collected packages: jraph\n",
            "Successfully installed jraph-0.0.6.dev0\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from flax) (1.25.2)\n",
            "Requirement already satisfied: jax>=0.4.19 in /usr/local/lib/python3.10/dist-packages (from flax) (0.4.23)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax) (1.0.7)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.9)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax) (13.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax) (4.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax) (6.0.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax) (1.11.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (2.16.1)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.1.85)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.4.23+cuda12.cudnn89)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (3.20.3)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.7->optax->flax) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.1.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.17.0)\n",
            "Collecting dm-haiku\n",
            "  Downloading dm_haiku-0.0.11-py3-none-any.whl (370 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.0/371.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from dm-haiku) (1.4.0)\n",
            "Collecting jmp>=0.0.2 (from dm-haiku)\n",
            "  Downloading jmp-0.0.4-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from dm-haiku) (1.25.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from dm-haiku) (0.9.0)\n",
            "Requirement already satisfied: flax>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from dm-haiku) (0.8.1)\n",
            "Requirement already satisfied: jax>=0.4.19 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku) (0.4.23)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku) (1.0.7)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku) (0.1.9)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku) (13.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku) (4.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.1->dm-haiku) (6.0.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax>=0.7.1->dm-haiku) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax>=0.7.1->dm-haiku) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax>=0.7.1->dm-haiku) (1.11.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.1->dm-haiku) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.1->dm-haiku) (2.16.1)\n",
            "Requirement already satisfied: chex>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.7.1->dm-haiku) (0.1.85)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.7.1->dm-haiku) (0.4.23+cuda12.cudnn89)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.1->dm-haiku) (1.6.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.1->dm-haiku) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.1->dm-haiku) (3.20.3)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.7->optax->flax>=0.7.1->dm-haiku) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.1->dm-haiku) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.1->dm-haiku) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.1->dm-haiku) (6.1.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.1->dm-haiku) (3.17.0)\n",
            "Installing collected packages: jmp, dm-haiku\n",
            "Successfully installed dm-haiku-0.0.11 jmp-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx1WEkVx3DNt",
        "outputId": "479088d6-eed9-49c1-ded7-c168d42a5a77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-5d7135309d37>:12: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  set_matplotlib_formats('svg', 'pdf') # For export\n",
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "import pickle\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "sns.set()\n",
        "#JRAPH\n",
        "%matplotlib inline\n",
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.tree_util as tree\n",
        "import jraph\n",
        "import flax\n",
        "import haiku as hk\n",
        "import optax\n",
        "import pickle\n",
        "import numpy as onp\n",
        "import networkx as nx\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
        "\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial7\"\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "# Create a DeviceArray from a regular NumPy array\n",
        "arr = jnp.array([1, 2, 3, 4, 5])\n",
        "\n",
        "# Or create a DeviceArray using jax.device_put\n",
        "from jax import device_put\n",
        "arr = device_put([1, 2, 3, 4, 5])\n"
      ],
      "metadata": {
        "id": "RVcDlkX79JvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX29soJh3DNv"
      },
      "source": [
        "We also have a few pre-trained models we download below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vGeRvqQ3DNw",
        "outputId": "91cf142a-186a-49ce-ea4f-4e30a248dbdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/NodeLevelMLP.ckpt...\n",
            "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/NodeLevelGNN.ckpt...\n",
            "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/GraphLevelGraphConv.ckpt...\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "# Github URL where saved models are stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/\"\n",
        "# Files to download\n",
        "pretrained_files = [\"NodeLevelMLP.ckpt\", \"NodeLevelGNN.ckpt\", \"GraphLevelGraphConv.ckpt\"]\n",
        "\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for file_name in pretrained_files:\n",
        "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
        "    if \"/\" in file_name:\n",
        "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdlGlAmv3DNw"
      },
      "source": [
        "## Graph Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnmxOW6q3DNx"
      },
      "source": [
        "### Graph representation\n",
        "\n",
        "Before starting the discussion of specific neural network operations on graphs, we should consider how to represent a graph. Mathematically, a graph $\\mathcal{G}$ is defined as a tuple of a set of nodes/vertices $V$, and a set of edges/links $E$: $\\mathcal{G}=(V,E)$. Each edge is a pair of two vertices, and represents a connection between them. For instance, let's look at the following graph:\n",
        "\n",
        "<center width=\"100%\" style=\"padding:10px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/example_graph.svg?raw=1\" width=\"250px\"></center>\n",
        "\n",
        "The vertices are $V=\\{1,2,3,4\\}$, and edges $E=\\{(1,2), (2,3), (2,4), (3,4)\\}$. Note that for simplicity, we assume the graph to be undirected and hence don't add mirrored pairs like $(2,1)$. In application, vertices and edge can often have specific attributes, and edges can even be directed. The question is how we could represent this diversity in an efficient way for matrix operations. Usually, for the edges, we decide between two variants: an adjacency matrix, or a list of paired vertex indices.\n",
        "\n",
        "The **adjacency matrix** $A$ is a square matrix whose elements indicate whether pairs of vertices are adjacent, i.e. connected, or not. In the simplest case, $A_{ij}$ is 1 if there is a connection from node $i$ to $j$, and otherwise 0. If we have edge attributes or different categories of edges in a graph, this information can be added to the matrix as well. For an undirected graph, keep in mind that $A$ is a symmetric matrix ($A_{ij}=A_{ji}$). For the example graph above, we have the following adjacency matrix:\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix}\n",
        "    0 & 1 & 0 & 0\\\\\n",
        "    1 & 0 & 1 & 1\\\\\n",
        "    0 & 1 & 0 & 1\\\\\n",
        "    0 & 1 & 1 & 0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "While expressing a graph as a list of edges is more efficient in terms of memory and (possibly) computation, using an adjacency matrix is more intuitive and simpler to implement. In our implementations below, we will rely on the adjacency matrix to keep the code simple. However, common libraries use edge lists, which we will discuss later more.\n",
        "Alternatively, we could also use the list of edges to define a sparse adjacency matrix with which we can work as if it was a dense matrix, but allows more memory-efficient operations. PyTorch supports this with the sub-package `torch.sparse` ([documentation](https://pytorch.org/docs/stable/sparse.html)) which is however still in a beta-stage (API might change in future)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5xPrgQv3DNy"
      },
      "source": [
        "### Graph Convolutions\n",
        "\n",
        "Graph Convolutional Networks have been introduced by [Kipf et al.](https://openreview.net/pdf?id=SJU4ayYgl) in 2016 at the University of Amsterdam. He also wrote a great [blog post](https://tkipf.github.io/graph-convolutional-networks/) about this topic, which is recommended if you want to read about GCNs from a different perspective. GCNs are similar to convolutions in images in the sense that the \"filter\" parameters are typically shared over all locations in the graph. At the same time, GCNs rely on message passing methods, which means that vertices exchange information with the neighbors, and send \"messages\" to each other. Before looking at the math, we can try to visually understand how GCNs work. The first step is that each node creates a feature vector that represents the message it wants to send to all its neighbors. In the second step, the messages are sent to the neighbors, so that a node receives one message per adjacent node. Below we have visualized the two steps for our example graph.\n",
        "\n",
        "<center width=\"100%\" style=\"padding:10px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/graph_message_passing.svg?raw=1\" width=\"700px\"></center>\n",
        "\n",
        "If we want to formulate that in more mathematical terms, we need to first decide how to combine all the messages a node receives. As the number of messages vary across nodes, we need an operation that works for any number. Hence, the usual way to go is to sum or take the mean. Given the previous features of nodes $H^{(l)}$, the GCN layer is defined as follows:\n",
        "\n",
        "$$H^{(l+1)} = \\sigma\\left(\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}H^{(l)}W^{(l)}\\right)$$\n",
        "\n",
        "$W^{(l)}$ is the weight parameters with which we transform the input features into messages ($H^{(l)}W^{(l)}$). To the adjacency matrix $A$ we add the identity matrix so that each node sends its own message also to itself: $\\hat{A}=A+I$. Finally, to take the average instead of summing, we calculate the matrix $\\hat{D}$ which is a diagonal matrix with $D_{ii}$ denoting the number of neighbors node $i$ has. $\\sigma$ represents an arbitrary activation function, and not necessarily the sigmoid (usually a ReLU-based activation function is used in GNNs).\n",
        "\n",
        "When implementing the GCN layer in PyTorch, we can take advantage of the flexible operations on tensors. Instead of defining a matrix $\\hat{D}$, we can simply divide the summed messages by the number of neighbors afterward. Additionally, we replace the weight matrix with a linear layer, which additionally allows us to add a bias. Written as a PyTorch module, the GCN layer is defined as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivUzUtf_3DNy"
      },
      "outputs": [],
      "source": [
        "class GCNLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, c_out):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(c_in, c_out)\n",
        "\n",
        "    def forward(self, node_feats, adj_matrix):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
        "            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
        "                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections.\n",
        "                         Shape: [batch_size, num_nodes, num_nodes]\n",
        "        \"\"\"\n",
        "        # Num neighbours = number of incoming edges\n",
        "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
        "        node_feats = self.projection(node_feats)\n",
        "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
        "        node_feats = node_feats / num_neighbours\n",
        "        return node_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SEyo7U63DNz"
      },
      "source": [
        "To further understand the GCN layer, we can apply it to our example graph above. First, let's specify some node features and the adjacency matrix with added self-connections:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxniHjyX3DNz",
        "outputId": "5c2de3c4-619b-40f9-d636-abab484c790d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node features:\n",
            " tensor([[[0., 1.],\n",
            "         [2., 3.],\n",
            "         [4., 5.],\n",
            "         [6., 7.]]])\n",
            "\n",
            "Adjacency matrix:\n",
            " tensor([[[1., 1., 0., 0.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.]]])\n"
          ]
        }
      ],
      "source": [
        "node_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
        "adj_matrix = torch.Tensor([[[1, 1, 0, 0],\n",
        "                            [1, 1, 1, 1],\n",
        "                            [0, 1, 1, 1],\n",
        "                            [0, 1, 1, 1]]])\n",
        "\n",
        "print(\"Node features:\\n\", node_feats)\n",
        "print(\"\\nAdjacency matrix:\\n\", adj_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEq7UpwU3DN0"
      },
      "source": [
        "Next, let's apply a GCN layer to it. For simplicity, we initialize the linear weight matrix as an identity matrix so that the input features are equal to the messages. This makes it easier for us to verify the message passing operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10c4MWpq3DN0",
        "outputId": "00566567-5df8-4790-f723-2414f2a825ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.]]])\n",
            "Input features tensor([[[0., 1.],\n",
            "         [2., 3.],\n",
            "         [4., 5.],\n",
            "         [6., 7.]]])\n",
            "Output features tensor([[[1., 2.],\n",
            "         [3., 4.],\n",
            "         [4., 5.],\n",
            "         [4., 5.]]])\n"
          ]
        }
      ],
      "source": [
        "layer = GCNLayer(c_in=2, c_out=2)\n",
        "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
        "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
        "\n",
        "with torch.no_grad():\n",
        "    out_feats = layer(node_feats, adj_matrix)\n",
        "\n",
        "print(\"Adjacency matrix\", adj_matrix)\n",
        "print(\"Input features\", node_feats)\n",
        "print(\"Output features\", out_feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhrqYsDW3DN0"
      },
      "source": [
        "As we can see, the first node's output values are the average of itself and the second node. Similarly, we can verify all other nodes. However, in a GNN, we would also want to allow feature exchange between nodes beyond its neighbors. This can be achieved by applying multiple GCN layers, which gives us the final layout of a GNN. The GNN can be build up by a sequence of GCN layers and non-linearities such as ReLU. For a visualization, see below (figure credit - [Thomas Kipf, 2016](https://tkipf.github.io/graph-convolutional-networks/)).\n",
        "\n",
        "<center width=\"100%\" style=\"padding: 10px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/gcn_network.png?raw=1\" width=\"600px\"></center>\n",
        "\n",
        "However, one issue we can see from looking at the example above is that the output features for nodes 3 and 4 are the same because they have the same adjacent nodes (including itself). Therefore, GCN layers can make the network forget node-specific information if we just take a mean over all messages. Multiple possible improvements have been proposed. While the simplest option might be using residual connections, the more common approach is to either weigh the self-connections higher or define a separate weight matrix for the self-connections. Alternatively, we can re-visit a concept from the last tutorial: attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6jYdWgP3DN1"
      },
      "source": [
        "### Graph Attention\n",
        "\n",
        "If you remember from the last tutorial, attention describes a weighted average of multiple elements with the weights dynamically computed based on an input query and elements' keys (if you haven't read Tutorial 6 yet, it is recommended to at least go through the very first section called [What is Attention?](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html#What-is-Attention?)). This concept can be similarly applied to graphs, one of such is the Graph Attention Network (called GAT, proposed by [Velickovic et al., 2017](https://arxiv.org/abs/1710.10903)). Similarly to the GCN, the graph attention layer creates a message for each node using a linear layer/weight matrix. For the attention part, it uses the message from the node itself as a query, and the messages to average as both keys and values (note that this also includes the message to itself). The score function $f_{attn}$ is implemented as a one-layer MLP which maps the query and key to a single value. The MLP looks as follows (figure credit - [Velickovic et al.](https://arxiv.org/abs/1710.10903)):\n",
        "\n",
        "<center width=\"100%\" style=\"padding:10px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/graph_attention_MLP.svg?raw=1\" width=\"250px\"></center>\n",
        "\n",
        "$h_i$ and $h_j$ are the original features from node $i$ and $j$ respectively, and represent the messages of the layer with $\\mathbf{W}$ as weight matrix. $\\mathbf{a}$ is the weight matrix of the MLP, which has the shape $[1,2\\times d_{\\text{message}}]$, and $\\alpha_{ij}$ the final attention weight from node $i$ to $j$. The calculation can be described as follows:\n",
        "\n",
        "$$\\alpha_{ij} = \\frac{\\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_j\\right]\\right)\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_k\\right]\\right)\\right)}$$\n",
        "\n",
        "The operator $||$ represents the concatenation, and $\\mathcal{N}_i$ the indices of the neighbors of node $i$. Note that in contrast to usual practice, we apply a non-linearity (here LeakyReLU) before the softmax over elements. Although it seems like a minor change at first, it is crucial for the attention to depend on the original input. Specifically, let's remove the non-linearity for a second, and try to simplify the expression:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\alpha_{ij} & = \\frac{\\exp\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_j\\right]\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_k\\right]\\right)}\\\\[5pt]\n",
        "    & = \\frac{\\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i+\\mathbf{a}_{:,d/2:}\\mathbf{W}h_j\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i+\\mathbf{a}_{:,d/2:}\\mathbf{W}h_k\\right)}\\\\[5pt]\n",
        "    & = \\frac{\\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i\\right)\\cdot\\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_j\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i\\right)\\cdot\\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_k\\right)}\\\\[5pt]\n",
        "    & = \\frac{\\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_j\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_k\\right)}\\\\\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "We can see that without the non-linearity, the attention term with $h_i$ actually cancels itself out, resulting in the attention being independent of the node itself. Hence, we would have the same issue as the GCN of creating the same output features for nodes with the same neighbors. This is why the LeakyReLU is crucial and adds some dependency on $h_i$ to the attention.\n",
        "\n",
        "Once we obtain all attention factors, we can calculate the output features for each node by performing the weighted average:\n",
        "\n",
        "$$h_i'=\\sigma\\left(\\sum_{j\\in\\mathcal{N}_i}\\alpha_{ij}\\mathbf{W}h_j\\right)$$\n",
        "\n",
        "$\\sigma$ is yet another non-linearity, as in the GCN layer. Visually, we can represent the full message passing in an attention layer as follows (figure credit - [Velickovic et al.](https://arxiv.org/abs/1710.10903)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/graph_attention.jpeg?raw=1\" width=\"400px\"></center>\n",
        "\n",
        "To increase the expressiveness of the graph attention network, [Velickovic et al.](https://arxiv.org/abs/1710.10903) proposed to extend it to multiple heads similar to the Multi-Head Attention block in Transformers. This results in $N$ attention layers being applied in parallel. In the image above, it is visualized as three different colors of arrows (green, blue, and purple) that are afterward concatenated. The average is only applied for the very final prediction layer in a network.\n",
        "\n",
        "After having discussed the graph attention layer in detail, we can implement it below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEMWnHZA3DN1"
      },
      "outputs": [],
      "source": [
        "class GATLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Dimensionality of input features\n",
        "            c_out - Dimensionality of output features\n",
        "            num_heads - Number of heads, i.e. attention mechanisms to apply in parallel. The\n",
        "                        output features are equally split up over the heads if concat_heads=True.\n",
        "            concat_heads - If True, the output of the different heads is concatenated instead of averaged.\n",
        "            alpha - Negative slope of the LeakyReLU activation.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.concat_heads = concat_heads\n",
        "        if self.concat_heads:\n",
        "            assert c_out % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
        "            c_out = c_out // num_heads\n",
        "\n",
        "        # Sub-modules and parameters needed in the layer\n",
        "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
        "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * c_out)) # One per head\n",
        "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
        "\n",
        "        # Initialization from the original implementation\n",
        "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            node_feats - Input features of the node. Shape: [batch_size, c_in]\n",
        "            adj_matrix - Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
        "            print_attn_probs - If True, the attention weights are printed during the forward pass (for debugging purposes)\n",
        "        \"\"\"\n",
        "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
        "\n",
        "        # Apply linear layer and sort nodes by head\n",
        "        node_feats = self.projection(node_feats)\n",
        "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
        "\n",
        "        # We need to calculate the attention logits for every edge in the adjacency matrix\n",
        "        # Doing this on all possible combinations of nodes is very expensive\n",
        "        # => Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges\n",
        "        edges = adj_matrix.nonzero(as_tuple=False) # Returns indices where the adjacency matrix is not 0 => edges\n",
        "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
        "        edge_indices_row = edges[:,0] * num_nodes + edges[:,1]\n",
        "        edge_indices_col = edges[:,0] * num_nodes + edges[:,2]\n",
        "        a_input = torch.cat([\n",
        "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
        "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
        "        ], dim=-1) # Index select returns a tensor with node_feats_flat being indexed at the desired positions along dim=0\n",
        "\n",
        "        # Calculate attention MLP output (independent for each head)\n",
        "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a)\n",
        "        attn_logits = self.leakyrelu(attn_logits)\n",
        "\n",
        "        # Map list of attention values back into a matrix\n",
        "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape+(self.num_heads,)).fill_(-9e15)\n",
        "        attn_matrix[adj_matrix[...,None].repeat(1,1,1,self.num_heads) == 1] = attn_logits.reshape(-1)\n",
        "\n",
        "        # Weighted average of attention\n",
        "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
        "        if print_attn_probs:\n",
        "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
        "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
        "\n",
        "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
        "        if self.concat_heads:\n",
        "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
        "        else:\n",
        "            node_feats = node_feats.mean(dim=2)\n",
        "\n",
        "        return node_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8Zoqq4m3DN1"
      },
      "source": [
        "Again, we can apply the graph attention layer on our example graph above to understand the dynamics better. As before, the input layer is initialized as an identity matrix, but we set $\\mathbf{a}$ to be a vector of arbitrary numbers to obtain different attention values. We use two heads to show the parallel, independent attention mechanisms working in the layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIOp44Ta3DN2",
        "outputId": "eb5f9c5d-dd1d-43ae-9461-ad8644e8db1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention probs\n",
            " tensor([[[[0.3543, 0.6457, 0.0000, 0.0000],\n",
            "          [0.1096, 0.1450, 0.2642, 0.4813],\n",
            "          [0.0000, 0.1858, 0.2885, 0.5257],\n",
            "          [0.0000, 0.2391, 0.2696, 0.4913]],\n",
            "\n",
            "         [[0.5100, 0.4900, 0.0000, 0.0000],\n",
            "          [0.2975, 0.2436, 0.2340, 0.2249],\n",
            "          [0.0000, 0.3838, 0.3142, 0.3019],\n",
            "          [0.0000, 0.4018, 0.3289, 0.2693]]]])\n",
            "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.]]])\n",
            "Input features tensor([[[0., 1.],\n",
            "         [2., 3.],\n",
            "         [4., 5.],\n",
            "         [6., 7.]]])\n",
            "Output features tensor([[[1.2913, 1.9800],\n",
            "         [4.2344, 3.7725],\n",
            "         [4.6798, 4.8362],\n",
            "         [4.5043, 4.7351]]])\n"
          ]
        }
      ],
      "source": [
        "layer = GATLayer(2, 2, num_heads=2)\n",
        "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
        "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
        "layer.a.data = torch.Tensor([[-0.2, 0.3], [0.1, -0.1]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    out_feats = layer(node_feats, adj_matrix, print_attn_probs=True)\n",
        "\n",
        "print(\"Adjacency matrix\", adj_matrix)\n",
        "print(\"Input features\", node_feats)\n",
        "print(\"Output features\", out_feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCRwd4zx3DN2"
      },
      "source": [
        "We recommend that you try to calculate the attention matrix at least for one head and one node for yourself. The entries are 0 where there does not exist an edge between $i$ and $j$. For the others, we see a diverse set of attention probabilities. Moreover, the output features of node 3 and 4 are now different although they have the same neighbors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fP3K12r3DN2"
      },
      "source": [
        "## PyTorch Geometric\n",
        "\n",
        "We had mentioned before that implementing graph networks with adjacency matrix is simple and straight-forward but can be computationally expensive for large graphs. Many real-world graphs can reach over 200k nodes, for which adjacency matrix-based implementations fail. There are a lot of optimizations possible when implementing GNNs, and luckily, there exist packages that provide such layers. The most popular packages for PyTorch are [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/) and the [Deep Graph Library](https://www.dgl.ai/) (the latter being actually framework agnostic). Which one to use depends on the project you are planning to do and personal taste. In this tutorial, we will look at PyTorch Geometric as part of the PyTorch family. Similar to PyTorch Lightning, PyTorch Geometric is not installed by default on GoogleColab (and actually also not in our `dl2021` environment due to many dependencies that would be unnecessary for the practicals). Hence, let's import and/or install it below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQQOtwJ93DN2",
        "outputId": "6e1cbe52-9d6a-4159-b2c6-e522e373e48d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt21cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.25.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt21cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_cluster-1.6.3%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.25.2)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt21cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (932 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m932.1/932.1 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt21cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.4.0\n"
          ]
        }
      ],
      "source": [
        "# torch geometric\n",
        "try:\n",
        "    import torch_geometric\n",
        "except ModuleNotFoundError:\n",
        "    # Installing torch geometric packages with specific CUDA+PyTorch version.\n",
        "    # See https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html for details\n",
        "    TORCH = torch.__version__.split('+')[0]\n",
        "    CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "\n",
        "    !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-geometric\n",
        "    import torch_geometric\n",
        "import torch_geometric.nn as geom_nn\n",
        "import torch_geometric.data as geom_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-R_ahSJ3DN3"
      },
      "source": [
        "PyTorch Geometric provides us a set of common graph layers, including the GCN and GAT layer we implemented above. Additionally, similar to PyTorch's torchvision, it provides the common graph datasets and transformations on those to simplify training. Compared to our implementation above, PyTorch Geometric uses a list of index pairs to represent the edges. The details of this library will be explored further in our experiments.\n",
        "\n",
        "In our tasks below, we want to allow us to pick from a multitude of graph layers. Thus, we define again below a dictionary to access those using a string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxYSN0M53DN3"
      },
      "outputs": [],
      "source": [
        "gnn_layer_by_name = {\n",
        "    \"GCN\": geom_nn.GCNConv,\n",
        "    \"GAT\": geom_nn.GATConv,\n",
        "    \"GraphConv\": geom_nn.GraphConv\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FKpa2RU3DN3"
      },
      "source": [
        "Additionally to GCN and GAT, we added the layer `geom_nn.GraphConv` ([documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GraphConv)). GraphConv is a GCN with a separate weight matrix for the self-connections. Mathematically, this would be:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_i^{(l+1)} = \\mathbf{W}^{(l + 1)}_1 \\mathbf{x}_i^{(l)} + \\mathbf{W}^{(\\ell + 1)}_2 \\sum_{j \\in \\mathcal{N}_i} \\mathbf{x}_j^{(l)}\n",
        "$$\n",
        "\n",
        "In this formula, the neighbor's messages are added instead of averaged. However, PyTorch Geometric provides the argument `aggr` to switch between summing, averaging, and max pooling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW-ljHAK3DN3"
      },
      "source": [
        "## Experiments on graph structures\n",
        "\n",
        "Tasks on graph-structured data can be grouped into three groups: node-level, edge-level and graph-level. The different levels describe on which level we want to perform classification/regression. We will discuss all three types in more detail below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JSedkTA3DN3"
      },
      "source": [
        "### Node-level tasks: Semi-supervised node classification\n",
        "\n",
        "Node-level tasks have the goal to classify nodes in a graph. Usually, we have given a single, large graph with >1000 nodes of which a certain amount of nodes are labeled. We learn to classify those labeled examples during training and try to generalize to the unlabeled nodes.\n",
        "\n",
        "A popular example that we will use in this tutorial is the Cora dataset, a citation network among papers. The Cora consists of 2708 scientific publications with links between each other representing the citation of one paper by another. The task is to classify each publication into one of seven classes. Each publication is represented by a bag-of-words vector. This means that we have a vector of 1433 elements for each publication, where a 1 at feature $i$ indicates that the $i$-th word of a pre-defined dictionary is in the article. Binary bag-of-words representations are commonly used when we need very simple encodings, and already have an intuition of what words to expect in a network. There exist much better approaches, but we will leave this to the NLP courses to discuss.\n",
        "\n",
        "We will load the dataset below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be3rsT3m3DN3",
        "outputId": "1452b186-685b-4e9d-a6b1-70bf4519ce61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "cora_dataset = torch_geometric.datasets.Planetoid(root=DATASET_PATH, name=\"Cora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi3j635c3DN4"
      },
      "source": [
        "Let's look at how PyTorch Geometric represents the graph data. Note that although we have a single graph, PyTorch Geometric returns a dataset for compatibility to other datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7k4vrn73DN4",
        "outputId": "9679d730-a3dd-47da-a332-d5d328894a30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "cora_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05PNrAnj3DN4"
      },
      "source": [
        "The graph is represented by a `Data` object ([documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data)) which we can access as a standard Python namespace. The edge index tensor is the list of edges in the graph and contains the mirrored version of each edge for undirected graphs. The `train_mask`, `val_mask`, and `test_mask` are boolean masks that indicate which nodes we should use for training, validation, and testing. The `x` tensor is the feature tensor of our 2708 publications, and `y` the labels for all nodes.\n",
        "\n",
        "After having seen the data, we can implement a simple graph neural network. The GNN applies a sequence of graph layers (GCN, GAT, or GraphConv), ReLU as activation function, and dropout for regularization. See below for the specific implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vHI-x9i3DN4"
      },
      "outputs": [],
      "source": [
        "class GNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, layer_name=\"GCN\", dp_rate=0.1, **kwargs):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Dimension of input features\n",
        "            c_hidden - Dimension of hidden features\n",
        "            c_out - Dimension of the output features. Usually number of classes in classification\n",
        "            num_layers - Number of \"hidden\" graph layers\n",
        "            layer_name - String of the graph layer to use\n",
        "            dp_rate - Dropout rate to apply throughout the network\n",
        "            kwargs - Additional arguments for the graph layer (e.g. number of heads for GAT)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        gnn_layer = gnn_layer_by_name[layer_name]\n",
        "\n",
        "        layers = []\n",
        "        in_channels, out_channels = c_in, c_hidden\n",
        "        for l_idx in range(num_layers-1):\n",
        "            layers += [\n",
        "                gnn_layer(in_channels=in_channels,\n",
        "                          out_channels=out_channels,\n",
        "                          **kwargs),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(dp_rate)\n",
        "            ]\n",
        "            in_channels = c_hidden\n",
        "        layers += [gnn_layer(in_channels=in_channels,\n",
        "                             out_channels=c_out,\n",
        "                             **kwargs)]\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x - Input features per node\n",
        "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
        "        \"\"\"\n",
        "        for l in self.layers:\n",
        "            # For graph layers, we need to add the \"edge_index\" tensor as additional input\n",
        "            # All PyTorch Geometric graph layer inherit the class \"MessagePassing\", hence\n",
        "            # we can simply check the class type.\n",
        "            if isinstance(l, geom_nn.MessagePassing):\n",
        "                x = l(x, edge_index)\n",
        "            else:\n",
        "                x = l(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGKeDHOe3DN5"
      },
      "source": [
        "Good practice in node-level tasks is to create an MLP baseline that is applied to each node independently. This way we can verify whether adding the graph information to the model indeed improves the prediction, or not. It might also be that the features per node are already expressive enough to clearly point towards a specific class. To check this, we implement a simple MLP below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo7yGgcJ3DN5"
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, dp_rate=0.1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Dimension of input features\n",
        "            c_hidden - Dimension of hidden features\n",
        "            c_out - Dimension of the output features. Usually number of classes in classification\n",
        "            num_layers - Number of hidden layers\n",
        "            dp_rate - Dropout rate to apply throughout the network\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        in_channels, out_channels = c_in, c_hidden\n",
        "        for l_idx in range(num_layers-1):\n",
        "            layers += [\n",
        "                nn.Linear(in_channels, out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(dp_rate)\n",
        "            ]\n",
        "            in_channels = c_hidden\n",
        "        layers += [nn.Linear(in_channels, c_out)]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x - Input features per node\n",
        "        \"\"\"\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abM_B4-x3DN5"
      },
      "source": [
        "Finally, we can merge the models into a PyTorch Lightning module which handles the training, validation, and testing for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNzUVres3DN5"
      },
      "outputs": [],
      "source": [
        "class NodeLevelGNN(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model_name, **model_kwargs):\n",
        "        super().__init__()\n",
        "        # Saving hyperparameters\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        if model_name == \"MLP\":\n",
        "            self.model = MLPModel(**model_kwargs)\n",
        "        else:\n",
        "            self.model = GNNModel(**model_kwargs)\n",
        "        self.loss_module = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, data, mode=\"train\"):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.model(x, edge_index)\n",
        "\n",
        "        # Only calculate the loss on the nodes corresponding to the mask\n",
        "        if mode == \"train\":\n",
        "            mask = data.train_mask\n",
        "        elif mode == \"val\":\n",
        "            mask = data.val_mask\n",
        "        elif mode == \"test\":\n",
        "            mask = data.test_mask\n",
        "        else:\n",
        "            assert False, f\"Unknown forward mode: {mode}\"\n",
        "\n",
        "        loss = self.loss_module(x[mask], data.y[mask])\n",
        "        acc = (x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum()\n",
        "        return loss, acc\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # We use SGD here, but Adam works as well\n",
        "        optimizer = optim.SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=2e-3)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, acc = self.forward(batch, mode=\"train\")\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"val\")\n",
        "        self.log('val_acc', acc)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"test\")\n",
        "        self.log('test_acc', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F3K9ktO3DN6"
      },
      "source": [
        "Additionally to the Lightning module, we define a training function below. As we have a single graph, we use a batch size of 1 for the data loader and share the same data loader for the train, validation, and test set (the mask is picked inside the Lightning module). Besides, we set the argument `enable_progress_bar` to False as it usually shows the progress per epoch, but an epoch only consists of a single step. The rest of the code is very similar to what we have seen in Tutorial 5 and 6 already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRIbYmVs3DN6"
      },
      "outputs": [],
      "source": [
        "def train_node_classifier(model_name, dataset, **model_kwargs):\n",
        "    pl.seed_everything(42)\n",
        "    node_data_loader = geom_data.DataLoader(dataset, batch_size=1)\n",
        "\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    root_dir = os.path.join(CHECKPOINT_PATH, \"NodeLevel\" + model_name)\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=200,\n",
        "                         enable_progress_bar=False) # False because epoch size is 1\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"NodeLevel{model_name}.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Found pretrained model, loading...\")\n",
        "        model = NodeLevelGNN.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        pl.seed_everything()\n",
        "        model = NodeLevelGNN(model_name=model_name, c_in=dataset.num_node_features, c_out=dataset.num_classes, **model_kwargs)\n",
        "        trainer.fit(model, node_data_loader, node_data_loader)\n",
        "        model = NodeLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "    # Test best model on the test set\n",
        "    test_result = trainer.test(model, node_data_loader, verbose=False)\n",
        "    batch = next(iter(node_data_loader))\n",
        "    batch = batch.to(model.device)\n",
        "    _, train_acc = model.forward(batch, mode=\"train\")\n",
        "    _, val_acc = model.forward(batch, mode=\"val\")\n",
        "    result = {\"train\": train_acc,\n",
        "              \"val\": val_acc,\n",
        "              \"test\": test_result[0]['test_acc']}\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVQ0HPcp3DN6"
      },
      "source": [
        "Finally, we can train our models. First, let's train the simple MLP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hGy-Yj53DOD"
      },
      "outputs": [],
      "source": [
        "# Small function for printing the test scores\n",
        "def print_results(result_dict):\n",
        "    if \"train\" in result_dict:\n",
        "        print(f\"Train accuracy: {(100.0*result_dict['train']):4.2f}%\")\n",
        "    if \"val\" in result_dict:\n",
        "        print(f\"Val accuracy:   {(100.0*result_dict['val']):4.2f}%\")\n",
        "    print(f\"Test accuracy:  {(100.0*result_dict['test']):4.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvfNsbVJ3DOD",
        "outputId": "6ae597bf-c075-4293-eea4-4b72eb92338b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.2.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../saved_models/tutorial7/NodeLevelMLP.ckpt`\n",
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: ../saved_models/tutorial7/NodeLevelMLP/lightning_logs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found pretrained model, loading...\n",
            "Train accuracy: 97.14%\n",
            "Val accuracy:   54.60%\n",
            "Test accuracy:  60.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2708. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
          ]
        }
      ],
      "source": [
        "node_mlp_model, node_mlp_result = train_node_classifier(model_name=\"MLP\",\n",
        "                                                        dataset=cora_dataset,\n",
        "                                                        c_hidden=16,\n",
        "                                                        num_layers=2,\n",
        "                                                        dp_rate=0.1)\n",
        "\n",
        "print_results(node_mlp_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwqN-e4A3DOD"
      },
      "source": [
        "Although the MLP can overfit on the training dataset because of the high-dimensional input features, it does not perform too well on the test set. Let's see if we can beat this score with our graph networks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmvbaenX3DOE",
        "outputId": "a28b63d0-2ea9-46fa-adbe-6b64400fe0fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found pretrained model, loading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.2.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../saved_models/tutorial7/NodeLevelGNN.ckpt`\n",
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: ../saved_models/tutorial7/NodeLevelGNN/lightning_logs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 100.00%\n",
            "Val accuracy:   78.60%\n",
            "Test accuracy:  82.40%\n"
          ]
        }
      ],
      "source": [
        "node_gnn_model, node_gnn_result = train_node_classifier(model_name=\"GNN\",\n",
        "                                                        layer_name=\"GCN\",\n",
        "                                                        dataset=cora_dataset,\n",
        "                                                        c_hidden=16,\n",
        "                                                        num_layers=2,\n",
        "                                                        dp_rate=0.1)\n",
        "print_results(node_gnn_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6w8_Gke3DOE"
      },
      "source": [
        "As we would have hoped for, the GNN model outperforms the MLP by quite a margin. This shows that using the graph information indeed improves our predictions and lets us generalizes better.\n",
        "\n",
        "The hyperparameters in the model have been chosen to create a relatively small network. This is because the first layer with an input dimension of 1433 can be relatively expensive to perform for large graphs. In general, GNNs can become relatively expensive for very big graphs. This is why such GNNs either have a small hidden size or use a special batching strategy where we sample a connected subgraph of the big, original graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7JB_3mn3DOE"
      },
      "source": [
        "### Edge-level tasks: Link prediction\n",
        "\n",
        "In some applications, we might have to predict on an edge-level instead of node-level. The most common edge-level task in GNN is link prediction. Link prediction means that given a graph, we want to predict whether there will be/should be an edge between two nodes or not. For example, in a social network, this is used by Facebook and co to propose new friends to you. Again, graph level information can be crucial to perform this task. The output prediction is usually done by performing a similarity metric on the pair of node features, which should be 1 if there should be a link, and otherwise close to 0. To keep the tutorial short, we will not implement this task ourselves. Nevertheless, there are many good resources out there if you are interested in looking closer at this task.\n",
        "Tutorials and papers for this topic include:\n",
        "\n",
        "* [PyTorch Geometric example](https://github.com/rusty1s/pytorch_geometric/blob/master/examples/link_pred.py)\n",
        "* [Graph Neural Networks: A Review of Methods and Applications](https://arxiv.org/pdf/1812.08434.pdf), Zhou et al. 2019\n",
        "* [Link Prediction Based on Graph Neural Networks](https://papers.nips.cc/paper/2018/file/53f0d7c537d99b3824f0f99d62ea2428-Paper.pdf), Zhang and Chen, 2018."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoF5tR8h3DOE"
      },
      "source": [
        "### Graph-level tasks: Graph classification\n",
        "\n",
        "Finally, in this part of the tutorial, we will have a closer look at how to apply GNNs to the task of graph classification. The goal is to classify an entire graph instead of single nodes or edges. Therefore, we are also given a dataset of multiple graphs that we need to classify based on some structural graph properties. The most common task for graph classification is molecular property prediction, in which molecules are represented as graphs. Each atom is linked to a node, and edges in the graph are the bonds between atoms. For example, look at the figure below.\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/molecule_graph.svg?raw=1\" width=\"600px\"></center>\n",
        "\n",
        "On the left, we have an arbitrary, small molecule with different atoms, whereas the right part of the image shows the graph representation. The atom types are abstracted as node features (e.g. a one-hot vector), and the different bond types are used as edge features. For simplicity, we will neglect the edge attributes in this tutorial, but you can include by using methods like the [Relational Graph Convolution](https://arxiv.org/abs/1703.06103) that uses a different weight matrix for each edge type.\n",
        "\n",
        "The dataset we will use below is called the MUTAG dataset. It is a common small benchmark for graph classification algorithms, and contain 188 graphs with 18 nodes and 20 edges on average for each graph. The graph nodes have 7 different labels/atom types, and the binary graph labels represent \"their mutagenic effect on a specific gram negative bacterium\" (the specific meaning of the labels are not too important here). The dataset is part of a large collection of different graph classification datasets, known as the [TUDatasets](https://chrsmrrs.github.io/datasets/), which is directly accessible via `torch_geometric.datasets.TUDataset` ([documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.TUDataset)) in PyTorch Geometric. We can load the dataset below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXiVNnAF3DOF",
        "outputId": "ba604b93-570b-4619-c7ab-88fadfcdfc49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
            "Extracting ../data/MUTAG/MUTAG.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "tu_dataset = torch_geometric.datasets.TUDataset(root=DATASET_PATH, name=\"MUTAG\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNa5aCSq3DOF"
      },
      "source": [
        "Let's look at some statistics for the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKDLsV2o3DOF",
        "outputId": "a57f1bcb-b41f-4bbf-a1f0-accf368a9c7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data object: Data(x=[3371, 7], edge_index=[2, 7442], edge_attr=[7442, 4], y=[188])\n",
            "Length: 188\n",
            "Average label: 0.66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "print(\"Data object:\", tu_dataset.data)\n",
        "print(\"Length:\", len(tu_dataset))\n",
        "print(f\"Average label: {tu_dataset.data.y.float().mean().item():4.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxLTk65Z3DOF"
      },
      "source": [
        "The first line shows how the dataset stores different graphs. The nodes, edges, and labels of each graph are concatenated to one tensor, and the dataset stores the indices where to split the tensors correspondingly. The length of the dataset is the number of graphs we have, and the \"average label\" denotes the percentage of the graph with label 1. As long as the percentage is in the range of 0.5, we have a relatively balanced dataset. It happens quite often that graph datasets are very imbalanced, hence checking the class balance is always a good thing to do.\n",
        "\n",
        "Next, we will split our dataset into a training and test part. Note that we do not use a validation set this time because of the small size of the dataset. Therefore, our model might overfit slightly on the validation set due to the noise of the evaluation, but we still get an estimate of the performance on untrained data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a70b_Ci53DOG"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "tu_dataset.shuffle()\n",
        "train_dataset = tu_dataset[:150]\n",
        "test_dataset = tu_dataset[150:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fjkr-dF3DOG"
      },
      "source": [
        "When using a data loader, we encounter a problem with batching $N$ graphs. Each graph in the batch can have a different number of nodes and edges, and hence we would require a lot of padding to obtain a single tensor. Torch geometric uses a different, more efficient approach: we can view the $N$ graphs in a batch as a single large graph with concatenated node and edge list. As there is no edge between the $N$ graphs, running GNN layers on the large graph gives us the same output as running the GNN on each graph separately. Visually, this batching strategy is visualized below (figure credit - PyTorch Geometric team, [tutorial here](https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing#scrollTo=2owRWKcuoALo)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/torch_geometric_stacking_graphs.png?raw=1\" width=\"600px\"></center>\n",
        "\n",
        "The adjacency matrix is zero for any nodes that come from two different graphs, and otherwise according to the adjacency matrix of the individual graph. Luckily, this strategy is already implemented in torch geometric, and hence we can use the corresponding data loader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qePomupW3DOG"
      },
      "outputs": [],
      "source": [
        "graph_train_loader = geom_data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "graph_val_loader = geom_data.DataLoader(test_dataset, batch_size=64) # Additional loader if you want to change to a larger dataset\n",
        "graph_test_loader = geom_data.DataLoader(test_dataset, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFvChApM3DOG"
      },
      "source": [
        "Let's load a batch below to see the batching in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCs8L81V3DOG",
        "outputId": "f317d617-edfb-47c7-8846-efff56848837",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch: DataBatch(edge_index=[2, 1512], x=[687, 7], edge_attr=[1512, 4], y=[38], batch=[687], ptr=[39])\n",
            "Labels: tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0])\n",
            "Batch indices: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2])\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(graph_test_loader))\n",
        "print(\"Batch:\", batch)\n",
        "print(\"Labels:\", batch.y[:10])\n",
        "print(\"Batch indices:\", batch.batch[:40])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xajfzo53DOH"
      },
      "source": [
        "We have 38 graphs stacked together for the test dataset. The batch indices, stored in `batch`, show that the first 12 nodes belong to the first graph, the next 22 to the second graph, and so on. These indices are important for performing the final prediction. To perform a prediction over a whole graph, we usually perform a pooling operation over all nodes after running the GNN model. In this case, we will use the average pooling. Hence, we need to know which nodes should be included in which average pool. Using this pooling, we can already create our graph network below. Specifically, we re-use our class `GNNModel` from before, and simply add an average pool and single linear layer for the graph prediction task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPBMSLTa3DOH"
      },
      "outputs": [],
      "source": [
        "class GraphGNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Dimension of input features\n",
        "            c_hidden - Dimension of hidden features\n",
        "            c_out - Dimension of output features (usually number of classes)\n",
        "            dp_rate_linear - Dropout rate before the linear layer (usually much higher than inside the GNN)\n",
        "            kwargs - Additional arguments for the GNNModel object\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.GNN = GNNModel(c_in=c_in,\n",
        "                            c_hidden=c_hidden,\n",
        "                            c_out=c_hidden, # Not our prediction output yet!\n",
        "                            **kwargs)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(dp_rate_linear),\n",
        "            nn.Linear(c_hidden, c_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, batch_idx):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x - Input features per node\n",
        "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
        "            batch_idx - Index of batch element for each node\n",
        "        \"\"\"\n",
        "        x = self.GNN(x, edge_index)\n",
        "        x = geom_nn.global_mean_pool(x, batch_idx) # Average pooling\n",
        "        x = self.head(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3QItvL43DOH"
      },
      "source": [
        "Finally, we can create a PyTorch Lightning module to handle the training. It is similar to the modules we have seen before and does nothing surprising in terms of training. As we have a binary classification task, we use the Binary Cross Entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYVKMyAZ3DOI"
      },
      "outputs": [],
      "source": [
        "class GraphLevelGNN(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, **model_kwargs):\n",
        "        super().__init__()\n",
        "        # Saving hyperparameters\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.model = GraphGNNModel(**model_kwargs)\n",
        "        self.loss_module = nn.BCEWithLogitsLoss() if self.hparams.c_out == 1 else nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, data, mode=\"train\"):\n",
        "        x, edge_index, batch_idx = data.x, data.edge_index, data.batch\n",
        "        x = self.model(x, edge_index, batch_idx)\n",
        "        x = x.squeeze(dim=-1)\n",
        "\n",
        "        if self.hparams.c_out == 1:\n",
        "            preds = (x > 0).float()\n",
        "            data.y = data.y.float()\n",
        "        else:\n",
        "            preds = x.argmax(dim=-1)\n",
        "        loss = self.loss_module(x, data.y)\n",
        "        acc = (preds == data.y).sum().float() / preds.shape[0]\n",
        "        return loss, acc\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=1e-2, weight_decay=0.0) # High lr because of small dataset and small model\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, acc = self.forward(batch, mode=\"train\")\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"val\")\n",
        "        self.log('val_acc', acc)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"test\")\n",
        "        self.log('test_acc', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjbSQOIT3DOI"
      },
      "source": [
        "Below we train the model on our dataset. It resembles the typical training functions we have seen so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qaQSyJn3DOI"
      },
      "outputs": [],
      "source": [
        "def train_graph_classifier(model_name, **model_kwargs):\n",
        "    pl.seed_everything(42)\n",
        "\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    root_dir = os.path.join(CHECKPOINT_PATH, \"GraphLevel\" + model_name)\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=500,\n",
        "                         enable_progress_bar=False)\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"GraphLevel{model_name}.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Found pretrained model, loading...\")\n",
        "        model = GraphLevelGNN.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        pl.seed_everything(42)\n",
        "        model = GraphLevelGNN(c_in=tu_dataset.num_node_features,\n",
        "                              c_out=1 if tu_dataset.num_classes==2 else tu_dataset.num_classes,\n",
        "                              **model_kwargs)\n",
        "        trainer.fit(model, graph_train_loader, graph_val_loader)\n",
        "        model = GraphLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "    # Test best model on validation and test set\n",
        "    train_result = trainer.test(model, graph_train_loader, verbose=False)\n",
        "    test_result = trainer.test(model, graph_test_loader, verbose=False)\n",
        "    result = {\"test\": test_result[0]['test_acc'], \"train\": train_result[0]['test_acc']}\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZARGBO73DOI"
      },
      "source": [
        "Finally, let's perform the training and testing. Feel free to experiment with different GNN layers, hyperparameters, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "8S84kWuk3DOI",
        "outputId": "9abc42ac-c444-4d4b-dd98-5182cb03874a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.2.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../saved_models/tutorial7/GraphLevelGraphConv.ckpt`\n",
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: ../saved_models/tutorial7/GraphLevelGraphConv/lightning_logs\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found pretrained model, loading...\n"
          ]
        }
      ],
      "source": [
        "model, result = train_graph_classifier(model_name=\"GraphConv\",\n",
        "                                       c_hidden=256,\n",
        "                                       layer_name=\"GraphConv\",\n",
        "                                       num_layers=3,\n",
        "                                       dp_rate_linear=0.5,\n",
        "                                       dp_rate=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9IvDVMr3DOI",
        "outputId": "599e3626-4f69-445d-d3c5-509370648b29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train performance: 94.27%\n",
            "Test performance:  92.11%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train performance: {100.0*result['train']:4.2f}%\")\n",
        "print(f\"Test performance:  {100.0*result['test']:4.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jraph**"
      ],
      "metadata": {
        "id": "mO0ilv8-4CBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_jraph_to_networkx_graph(jraph_graph: jraph.GraphsTuple) -> nx.Graph:\n",
        "  nodes, edges, receivers, senders, _, _, _ = jraph_graph\n",
        "  nx_graph = nx.DiGraph()\n",
        "  if nodes is None:\n",
        "    for n in range(jraph_graph.n_node[0]):\n",
        "      nx_graph.add_node(n)\n",
        "  else:\n",
        "    for n in range(jraph_graph.n_node[0]):\n",
        "      nx_graph.add_node(n, node_feature=nodes[n])\n",
        "  if edges is None:\n",
        "    for e in range(jraph_graph.n_edge[0]):\n",
        "      nx_graph.add_edge(int(senders[e]), int(receivers[e]))\n",
        "  else:\n",
        "    for e in range(jraph_graph.n_edge[0]):\n",
        "      nx_graph.add_edge(\n",
        "          int(senders[e]), int(receivers[e]), edge_feature=edges[e])\n",
        "  return nx_graph\n",
        "\n",
        "\n",
        "def draw_jraph_graph_structure(jraph_graph: jraph.GraphsTuple) -> None:\n",
        "  nx_graph = convert_jraph_to_networkx_graph(jraph_graph)\n",
        "  pos = nx.spring_layout(nx_graph)\n",
        "  nx.draw(\n",
        "      nx_graph, pos=pos, with_labels=True, node_size=500, font_color='yellow')"
      ],
      "metadata": {
        "id": "lpDE1Pm478Fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download jraph version of MUTAG.\n",
        "!wget -P /tmp/ https://storage.googleapis.com/dm-educational/assets/graph-nets/jraph_datasets/mutag.pickle\n",
        "with open('/tmp/mutag.pickle', 'rb') as f:\n",
        "  mutag_ds = pickle.load(f)"
      ],
      "metadata": {
        "id": "Ph71Tq8f7Y-g",
        "outputId": "e763fc77-4d49-4001-8b90-12273da95836",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-13 19:24:40--  https://storage.googleapis.com/dm-educational/assets/graph-nets/jraph_datasets/mutag.pickle\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.188.207, 64.233.189.207, 142.250.157.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.188.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 352981 (345K) [application/octet-stream]\n",
            "Saving to: ‘/tmp/mutag.pickle’\n",
            "\n",
            "mutag.pickle        100%[===================>] 344.71K   920KB/s    in 0.4s    \n",
            "\n",
            "2024-02-13 19:24:41 (920 KB/s) - ‘/tmp/mutag.pickle’ saved [352981/352981]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(mutag_ds)"
      ],
      "metadata": {
        "id": "8N0sOU8T7f0u",
        "outputId": "15478dc0-af22-47e6-bb1e-45c6f1c30ae8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "188"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the first graph\n",
        "g = mutag_ds[0]['input_graph']\n",
        "print(f'Number of nodes: {g.n_node[0]}')\n",
        "print(f'Number of edges: {g.n_edge[0]}')\n",
        "print(f'Node features shape: {g.nodes.shape}')\n",
        "print(f'Edge features shape: {g.edges.shape}')"
      ],
      "metadata": {
        "id": "NIkbP_e-7hQP",
        "outputId": "72cb5f0b-d19a-4a4c-b69a-f1b0d4754823",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 17\n",
            "Number of edges: 38\n",
            "Node features shape: (17, 7)\n",
            "Edge features shape: (38, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "draw_jraph_graph_structure(g)"
      ],
      "metadata": {
        "id": "4JY2MFZb7lVm",
        "outputId": "2697140f-5d76-4ede-c8d6-66bde650f4dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"475.2pt\" height=\"360pt\" viewBox=\"0 0 475.2 360\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-02-13T19:27:08.841592</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 360 \nL 475.2 360 \nL 475.2 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 412.334438 44.43608 \nQ 415.916328 47.48444 418.646785 49.808188 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 416.896826 45.692652 \nL 418.646785 49.808188 \nL 414.30438 48.738834 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 401.952944 48.213389 \nQ 400.434448 57.177659 399.102681 65.039599 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 401.742649 61.429811 \nL 399.102681 65.039599 \nL 397.798832 60.761751 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 419.499014 50.533476 \nQ 415.917123 47.485116 413.186666 45.161368 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 414.936625 49.276904 \nL 413.186666 45.161368 \nL 417.529071 46.230722 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 420.539501 66.091488 \nQ 410.169678 77.624445 400.547388 88.326022 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 404.709064 86.688812 \nL 400.547388 88.326022 \nL 401.734626 84.014356 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 399.797798 89.15969 \nQ 410.167621 77.626732 419.789911 66.925156 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 415.628235 68.562365 \nL 419.789911 66.925156 \nL 418.602673 71.236822 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path d=\"M 384.898954 105.826486 \nQ 373.405478 118.758998 362.654709 130.855813 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 366.806833 129.194528 \nL 362.654709 130.855813 \nL 363.816956 126.537343 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 361.909944 131.693826 \nQ 373.403419 118.761315 384.154188 106.664499 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 380.002064 108.325784 \nL 384.154188 106.664499 \nL 382.991942 110.982969 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 365.427907 137.754226 \nQ 379.489607 134.80598 392.457066 132.087158 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 388.131782 130.950532 \nL 392.457066 132.087158 \nL 388.952595 134.865409 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 346.28697 147.654486 \nQ 329.093154 163.60672 312.718947 178.798531 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 317.01155 177.544117 \nL 312.718947 178.798531 \nL 314.290978 174.6118 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 393.553314 131.857313 \nQ 379.491614 134.805559 366.524155 137.524381 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 370.849439 138.661007 \nL 366.524155 137.524381 \nL 370.028626 134.74613 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path d=\"M 402.923135 118.494213 \nQ 400.772763 103.365243 398.779723 89.343182 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 397.362511 93.584822 \nL 398.779723 89.343182 \nL 401.322708 93.021935 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 407.063375 140.444762 \nQ 410.172597 153.625263 413.025125 165.717597 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 414.053322 161.365263 \nL 413.025125 165.717597 \nL 410.160176 162.283639 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 398.916023 66.141512 \nQ 400.434519 57.177242 401.766286 49.315302 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 399.126317 52.92509 \nL 401.766286 49.315302 \nL 403.070135 53.59315 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 398.622002 88.233536 \nQ 400.772374 103.362506 402.765415 117.384567 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 404.182626 113.142927 \nL 402.765415 117.384567 \nL 400.22243 113.705814 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 413.28093 166.801995 \nQ 410.171708 153.621494 407.31918 141.52916 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 406.290983 145.881494 \nL 407.31918 141.52916 \nL 410.184129 144.963118 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path d=\"M 408.702209 186.278615 \nQ 399.01334 197.932313 390.039237 208.726296 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 394.134366 206.929098 \nL 390.039237 208.726296 \nL 391.058556 204.371874 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 389.322309 209.588611 \nQ 399.011178 197.934913 407.985281 187.14093 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 403.890152 188.938128 \nL 407.985281 187.14093 \nL 406.965962 191.495352 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path d=\"M 371.267407 220.636983 \nQ 348.517232 225.753713 326.857842 230.625115 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 331.199213 231.69866 \nL 326.857842 230.625115 \nL 330.321501 227.796145 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path d=\"M 325.76513 230.870876 \nQ 348.515306 225.754146 370.174695 220.882745 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 365.833324 219.8092 \nL 370.174695 220.882745 \nL 366.711036 223.711715 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path d=\"M 312.230769 222.457215 \nQ 309.280654 210.244749 306.593067 199.119057 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 305.588232 203.476844 \nL 306.593067 199.119057 \nL 309.476396 202.537596 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path d=\"M 304.919732 238.45486 \nQ 283.121927 249.709824 262.317547 260.451847 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 266.789306 260.39379 \nL 262.317547 260.451847 \nL 264.954157 256.839606 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path d=\"M 311.902482 179.556038 \nQ 329.096297 163.603804 345.470505 148.411993 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 341.177901 149.666406 \nL 345.470505 148.411993 \nL 343.898473 152.598724 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path d=\"M 306.329785 198.02916 \nQ 309.2799 210.241627 311.967487 221.367319 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 312.972321 217.009532 \nL 311.967487 221.367319 \nL 309.084158 217.948779 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path d=\"M 292.940682 190.184688 \nQ 270.667706 196.439469 249.471125 202.391972 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 253.862887 203.236027 \nL 249.471125 202.391972 \nL 252.781426 199.384995 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path d=\"M 248.392701 202.694819 \nQ 270.665677 196.440038 291.862258 190.487535 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 287.470496 189.64348 \nL 291.862258 190.487535 \nL 288.551957 193.494511 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path d=\"M 228.267563 211.825143 \nQ 214.327653 220.920005 201.324108 229.40395 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 205.766994 228.893292 \nL 201.324108 229.40395 \nL 203.581313 225.543247 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path d=\"M 200.38595 230.016036 \nQ 214.325861 220.921174 227.329405 212.437229 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 222.88652 212.947887 \nL 227.329405 212.437229 \nL 225.072201 216.297932 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path d=\"M 187.050128 246.574496 \nQ 183.666423 255.471618 180.680151 263.32373 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 183.971421 260.295936 \nL 180.680151 263.32373 \nL 180.232677 258.874037 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path d=\"M 180.282923 264.368201 \nQ 183.666628 255.471079 186.6529 247.618967 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 183.36163 250.646762 \nL 186.6529 247.618967 \nL 187.100373 252.06866 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path d=\"M 187.411731 273.52892 \nQ 213.845391 270.457399 239.16849 267.514923 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 234.964381 265.989972 \nL 239.16849 267.514923 \nL 235.426064 269.963239 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path d=\"M 165.609737 278.06973 \nQ 141.486498 285.399271 118.433006 292.403783 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 122.841674 293.154545 \nL 118.433006 292.403783 \nL 121.678816 289.327305 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 261.324448 260.964618 \nQ 283.122254 249.709655 303.926634 238.967632 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 299.454875 239.025688 \nL 303.926634 238.967632 \nL 301.290024 242.579873 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 240.284817 267.385209 \nQ 213.851156 270.456729 188.528058 273.399205 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 192.732166 274.924156 \nL 188.528058 273.399205 \nL 192.270483 270.950889 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 117.366357 292.727871 \nQ 141.489595 285.39833 164.543088 278.393818 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 160.134419 277.643057 \nL 164.543088 278.393818 \nL 161.297278 281.470296 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 108.328207 307.032698 \nQ 108.682581 309.391963 108.870884 310.645596 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 110.25454 306.392891 \nL 108.870884 310.645596 \nL 106.298914 306.987049 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_37\">\n    <path d=\"M 95.555045 297.192123 \nQ 76.928361 299.226354 59.413103 301.139206 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 63.606591 302.693124 \nL 59.413103 301.139206 \nL 63.17233 298.716767 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_38\">\n    <path d=\"M 109.03763 311.755711 \nQ 108.683255 309.396446 108.494952 308.142813 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 107.111296 312.395518 \nL 108.494952 308.142813 \nL 111.066922 311.80136 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"patch_39\">\n    <path d=\"M 58.299534 301.26082 \nQ 76.926218 299.226588 94.441477 297.313736 \n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: none; stroke: #1a1a1a; stroke-linecap: round\"/>\n    <path d=\"M 90.247989 295.759818 \nL 94.441477 297.313736 \nL 90.682249 299.736175 \nz\n\" clip-path=\"url(#p62e241a4a4)\" style=\"fill: #1a1a1a; stroke: #1a1a1a; stroke-linecap: round\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path id=\"ma97689dec7\" d=\"M 0 11.18034 \nC 2.965061 11.18034 5.80908 10.002309 7.905694 7.905694 \nC 10.002309 5.80908 11.18034 2.965061 11.18034 0 \nC 11.18034 -2.965061 10.002309 -5.80908 7.905694 -7.905694 \nC 5.80908 -10.002309 2.965061 -11.18034 0 -11.18034 \nC -2.965061 -11.18034 -5.80908 -10.002309 -7.905694 -7.905694 \nC -10.002309 -5.80908 -11.18034 -2.965061 -11.18034 0 \nC -11.18034 2.965061 -10.002309 5.80908 -7.905694 7.905694 \nC -5.80908 10.002309 -2.965061 11.18034 0 11.18034 \nz\n\" style=\"stroke: #1f78b4\"/>\n    </defs>\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <use xlink:href=\"#ma97689dec7\" x=\"403.820228\" y=\"37.190083\" style=\"fill: #1f78b4; stroke: #1f78b4\"/>\n     <use xlink:href=\"#ma97689dec7\" x=\"428.013223\" y=\"57.779473\" style=\"fill: #1f78b4; stroke: #1f78b4\"/>\n     <use xlink:href=\"#ma97689dec7\" x=\"392.324076\" y=\"97.471704\" style=\"fill: #1f78b4; stroke: #1f78b4\"/>\n     <use xlink:href=\"#ma97689dec7\" x=\"354.484822\" y=\"140.048608\" style=\"fill: #1f78b4; stroke: #1f78b4\"/>\n     <use xlink:href=\"#ma97689dec7\" x=\"404.496399\" y=\"129.562931\" style=\"fill: #1f78b4; stroke: #1f78b4\"/>\n     <use xlink:href=\"#ma97689dec7\" x=\"397.048739\" y=\"77.164818\" style=\"fill: #1f78b4; stroke: #1f78b4\"/>\n     <use xlink:href=\"#ma97689dec7\" x=\"415.847905\" y=\"177.683826\" style=\"fill: #1f78b4; stroke: #1f78b4\"/>\n     <use xlink:href=\"#ma97689dec7\" x=\"382.176613\" y=\"218.1834\" style=\"fill: #1f78b4; stroke: #1f78b4\"/>\n     <use xlink:href=\"#ma97689dec7\" x=\"314.855924\" y=\"233.32446\" style=\"fill: #1f78b4; stroke: #1f78b4\"/>\n     <use xlink:href=\"#ma97689dec7\" x=\"303.704629\" y=\"187.161916\" style=\"fill: #1f78b4; stroke: #1f78b4\"/>\n     <use xlink:href=\"#ma97689dec7\" x=\"237.628754\" y=\"205.717591\" style=\"fill: #1f78b4; stroke: #1f78b4\"/>\n     <use xlink:href=\"#ma97689dec7\" x=\"191.02476\" y=\"236.123588\" style=\"fill: #1f78b4; stroke: #1f78b4\"/>\n     <use xlink:href=\"#ma97689dec7\" x=\"176.308291\" y=\"274.81911\" style=\"fill: #1f78b4; stroke: #1f78b4\"/>\n     <use xlink:href=\"#ma97689dec7\" x=\"251.388257\" y=\"266.095019\" style=\"fill: #1f78b4; stroke: #1f78b4\"/>\n     <use xlink:href=\"#ma97689dec7\" x=\"106.667803\" y=\"295.978492\" style=\"fill: #1f78b4; stroke: #1f78b4\"/>\n     <use xlink:href=\"#ma97689dec7\" x=\"110.698034\" y=\"322.809917\" style=\"fill: #1f78b4; stroke: #1f78b4\"/>\n     <use xlink:href=\"#ma97689dec7\" x=\"47.186777\" y=\"302.474451\" style=\"fill: #1f78b4; stroke: #1f78b4\"/>\n    </g>\n   </g>\n   <g id=\"text_1\">\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <!-- 0 -->\n     <g style=\"fill: #ffff00\" transform=\"translate(400.002728 40.501333) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_2\">\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <!-- 1 -->\n     <g style=\"fill: #ffff00\" transform=\"translate(424.195723 61.090723) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-31\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_3\">\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <!-- 2 -->\n     <g style=\"fill: #ffff00\" transform=\"translate(388.506576 100.782954) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-32\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_4\">\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <!-- 3 -->\n     <g style=\"fill: #ffff00\" transform=\"translate(350.667322 143.359858) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-33\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_5\">\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <!-- 4 -->\n     <g style=\"fill: #ffff00\" transform=\"translate(400.678899 132.874181) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-34\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_6\">\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <!-- 5 -->\n     <g style=\"fill: #ffff00\" transform=\"translate(393.231239 80.476068) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_7\">\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <!-- 6 -->\n     <g style=\"fill: #ffff00\" transform=\"translate(412.030405 180.995076) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-36\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_8\">\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <!-- 7 -->\n     <g style=\"fill: #ffff00\" transform=\"translate(378.359113 221.49465) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-37\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_9\">\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <!-- 8 -->\n     <g style=\"fill: #ffff00\" transform=\"translate(311.038424 236.63571) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-38\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_10\">\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <!-- 9 -->\n     <g style=\"fill: #ffff00\" transform=\"translate(299.887129 190.473166) scale(0.12 -0.12)\">\n      <defs>\n       <path id=\"DejaVuSans-39\" d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-39\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_11\">\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <!-- 10 -->\n     <g style=\"fill: #ffff00\" transform=\"translate(229.993754 209.028841) scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_12\">\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <!-- 11 -->\n     <g style=\"fill: #ffff00\" transform=\"translate(183.38976 239.434838) scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_13\">\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <!-- 12 -->\n     <g style=\"fill: #ffff00\" transform=\"translate(168.673291 278.13036) scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_14\">\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <!-- 13 -->\n     <g style=\"fill: #ffff00\" transform=\"translate(243.753257 269.406269) scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_15\">\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <!-- 14 -->\n     <g style=\"fill: #ffff00\" transform=\"translate(99.032803 299.289742) scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_16\">\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <!-- 15 -->\n     <g style=\"fill: #ffff00\" transform=\"translate(103.063034 326.121167) scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_17\">\n    <g clip-path=\"url(#p62e241a4a4)\">\n     <!-- 16 -->\n     <g style=\"fill: #ffff00\" transform=\"translate(39.551777 305.785701) scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p62e241a4a4\">\n   <rect x=\"7.2\" y=\"7.2\" width=\"460.8\" height=\"345.6\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "application/pdf": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_simplified_gcn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  # Unpack GraphsTuple\n",
        "  nodes, _, receivers, senders, _, _, _ = graph\n",
        "\n",
        "  # 1. Update node features\n",
        "  # For simplicity, we will first use an identify function here, and replace it\n",
        "  # with a trainable MLP block later.\n",
        "  update_node_fn = lambda nodes: nodes\n",
        "  nodes = update_node_fn(nodes)\n",
        "\n",
        "  # 2. Aggregate node features over nodes in neighborhood\n",
        "  # Equivalent to jnp.sum(n_node), but jittable\n",
        "  total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
        "  aggregate_nodes_fn = jax.ops.segment_sum\n",
        "\n",
        "  # Compute new node features by aggregating messages from neighboring nodes\n",
        "  nodes = tree.tree_map(lambda x: aggregate_nodes_fn(x[senders], receivers,\n",
        "                                        total_num_nodes), nodes)\n",
        "  out_graph = graph._replace(nodes=nodes)\n",
        "  return out_graph"
      ],
      "metadata": {
        "id": "jqM2lSRu8Aj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = build_toy_graph()"
      ],
      "metadata": {
        "id": "Svc7er9H8ExG",
        "outputId": "2556033f-86be-4be5-df13-8c55a5b5f6a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'build_toy_graph' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-6979314f7db1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_toy_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'build_toy_graph' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download jraph version of Cora.\n",
        "!wget -P /tmp/ https://storage.googleapis.com/dm-educational/assets/graph-nets/jraph_datasets/cora.pickle\n",
        "with open('/tmp/cora.pickle', 'rb') as f:\n",
        "  cora_ds = pickle.load(f)"
      ],
      "metadata": {
        "id": "hCC0GXff4EmD",
        "outputId": "d43a8919-7965-4c0f-d0f9-7fb8e752c53b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-13 19:16:38--  https://storage.googleapis.com/dm-educational/assets/graph-nets/jraph_datasets/cora.pickle\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.188.207, 64.233.189.207, 108.177.97.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.188.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15618018 (15M) [application/octet-stream]\n",
            "Saving to: ‘/tmp/cora.pickle.2’\n",
            "\n",
            "cora.pickle.2       100%[===================>]  14.89M  17.4MB/s    in 0.9s    \n",
            "\n",
            "2024-02-13 19:16:39 (17.4 MB/s) - ‘/tmp/cora.pickle.2’ saved [15618018/15618018]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_test_split_edges(graph: jraph.GraphsTuple,\n",
        "                               val_perc: float = 0.05,\n",
        "                               test_perc: float = 0.1):\n",
        "  \"\"\"Split edges in input graph into train, val and test splits.\n",
        "\n",
        "  For val and test sets, also include negative edges.\n",
        "  Based on torch_geometric.utils.train_test_split_edges.\n",
        "  \"\"\"\n",
        "  mask = graph.senders < graph.receivers\n",
        "  senders = graph.senders[mask]\n",
        "  receivers = graph.receivers[mask]\n",
        "  num_val = int(val_perc * senders.shape[0])\n",
        "  num_test = int(test_perc * senders.shape[0])\n",
        "  permuted_indices = onp.random.permutation(range(senders.shape[0]))\n",
        "  senders = senders[permuted_indices]\n",
        "  receivers = receivers[permuted_indices]\n",
        "  if graph.edges is not None:\n",
        "    edges = graph.edges[permuted_indices]\n",
        "\n",
        "  val_senders = senders[:num_val]\n",
        "  val_receivers = receivers[:num_val]\n",
        "  if graph.edges is not None:\n",
        "    val_edges = edges[:num_val]\n",
        "\n",
        "  test_senders = senders[num_val:num_val + num_test]\n",
        "  test_receivers = receivers[num_val:num_val + num_test]\n",
        "  if graph.edges is not None:\n",
        "    test_edges = edges[num_val:num_val + num_test]\n",
        "\n",
        "  train_senders = senders[num_val + num_test:]\n",
        "  train_receivers = receivers[num_val + num_test:]\n",
        "  train_edges = None\n",
        "  if graph.edges is not None:\n",
        "    train_edges = edges[num_val + num_test:]\n",
        "\n",
        "  # make training edges undirected by adding reverse edges back in\n",
        "  train_senders_undir = jnp.concatenate((train_senders, train_receivers))\n",
        "  train_receivers_undir = jnp.concatenate((train_receivers, train_senders))\n",
        "  train_senders = train_senders_undir\n",
        "  train_receivers = train_receivers_undir\n",
        "\n",
        "  # Negative edges.\n",
        "  num_nodes = graph.n_node[0]\n",
        "  # Create a negative adjacency mask, s.t. mask[i, j] = True iff edge i->j does\n",
        "  # not exist in the original graph.\n",
        "  neg_adj_mask = onp.ones((num_nodes, num_nodes), dtype=onp.uint8)\n",
        "  # upper triangular part\n",
        "  neg_adj_mask = onp.triu(neg_adj_mask, k=1)\n",
        "  neg_adj_mask[graph.senders, graph.receivers] = 0\n",
        "  neg_adj_mask = neg_adj_mask.astype(bool)\n",
        "  neg_senders, neg_receivers = neg_adj_mask.nonzero()\n",
        "\n",
        "  perm = onp.random.permutation(range(len(neg_senders)))\n",
        "  neg_senders = neg_senders[perm]\n",
        "  neg_receivers = neg_receivers[perm]\n",
        "\n",
        "  val_neg_senders = neg_senders[:num_val]\n",
        "  val_neg_receivers = neg_receivers[:num_val]\n",
        "  test_neg_senders = neg_senders[num_val:num_val + num_test]\n",
        "  test_neg_receivers = neg_receivers[num_val:num_val + num_test]\n",
        "\n",
        "  train_graph = jraph.GraphsTuple(\n",
        "      nodes=graph.nodes,\n",
        "      edges=train_edges,\n",
        "      senders=train_senders,\n",
        "      receivers=train_receivers,\n",
        "      n_node=graph.n_node,\n",
        "      n_edge=jnp.array([len(train_senders)]),\n",
        "      globals=graph.globals)\n",
        "\n",
        "  return train_graph, neg_adj_mask, val_senders, val_receivers, val_neg_senders, val_neg_receivers, test_senders, test_receivers, test_neg_senders, test_neg_receivers"
      ],
      "metadata": {
        "id": "ZcvNQ1Ls5ySC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = cora_ds[0]['input_graph']\n",
        "\n",
        "train_graph, neg_adj_mask, val_pos_senders, val_pos_receivers, val_neg_senders, val_neg_receivers, test_pos_senders, test_pos_receivers, test_neg_senders, test_neg_receivers = train_val_test_split_edges(graph)"
      ],
      "metadata": {
        "id": "Vi02CgLb5_sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Train set: {train_graph.senders.shape[0]} positive edges, we will sample the same number of negative edges at runtime')\n",
        "print(f'Val set: {val_pos_senders.shape[0]} positive edges, {val_neg_senders.shape[0]} negative edges')\n",
        "print(f'Test set: {test_pos_senders.shape[0]} positive edges, {test_neg_senders.shape[0]} negative edges')\n",
        "print(f'Negative adjacency mask shape: {neg_adj_mask.shape}')\n",
        "print(f'Numbe of negative edges to sample from: {neg_adj_mask.sum()}')"
      ],
      "metadata": {
        "id": "puOPF3gD8gQv",
        "outputId": "36cb9b9a-5219-4bbc-c7d7-0943ce2e31b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: 8976 positive edges, we will sample the same number of negative edges at runtime\n",
            "Val set: 263 positive edges, 263 negative edges\n",
            "Test set: 527 positive edges, 527 negative edges\n",
            "Negative adjacency mask shape: (2708, 2708)\n",
            "Numbe of negative edges to sample from: 3660000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@jraph.concatenated_args\n",
        "def node_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Node update function for graph net.\"\"\"\n",
        "  net = hk.Sequential([hk.Linear(128), jax.nn.relu, hk.Linear(64)])\n",
        "  return net(feats)\n",
        "\n",
        "\n",
        "def net_fn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Network definition.\"\"\"\n",
        "  graph = graph._replace(globals=jnp.zeros([graph.n_node.shape[0], 1]))\n",
        "  net = jraph.GraphNetwork(\n",
        "      update_node_fn=node_update_fn, update_edge_fn=None, update_global_fn=None)\n",
        "  return net(graph)\n",
        "\n",
        "\n",
        "def decode(pred_graph: jraph.GraphsTuple, senders: jnp.ndarray,\n",
        "           receivers: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Given a set of candidate edges, take dot product of respective nodes.\n",
        "\n",
        "  Args:\n",
        "    pred_graph: input graph.\n",
        "    senders: Senders of candidate edges.\n",
        "    receivers: Receivers of candidate edges.\n",
        "\n",
        "  Returns:\n",
        "    For each edge, computes dot product of the features of the two nodes.\n",
        "\n",
        "  \"\"\"\n",
        "  return jnp.squeeze(\n",
        "      jnp.sum(pred_graph.nodes[senders] * pred_graph.nodes[receivers], axis=1))"
      ],
      "metadata": {
        "id": "KPSWYVuM8hx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_bce_with_logits_loss(x: jnp.ndarray, y: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Computes binary cross-entropy with logits loss.\n",
        "\n",
        "  Combines sigmoid and BCE, and uses log-sum-exp trick for numerical stability.\n",
        "  See https://stackoverflow.com/a/66909858 if you want to learn more.\n",
        "\n",
        "  Args:\n",
        "    x: Predictions (logits).\n",
        "    y: Labels.\n",
        "\n",
        "  Returns:\n",
        "    Binary cross-entropy loss with mean aggregation.\n",
        "\n",
        "  \"\"\"\n",
        "  max_val = jnp.clip(x, 0, None)\n",
        "  loss = x - x * y + max_val + jnp.log(\n",
        "      jnp.exp(-max_val) + jnp.exp((-x - max_val)))\n",
        "  return loss.mean()\n",
        "\n",
        "\n",
        "def compute_loss(params: hk.Params, graph: jraph.GraphsTuple,\n",
        "                 senders: jnp.ndarray, receivers: jnp.ndarray,\n",
        "                 labels: jnp.ndarray,\n",
        "                 net: hk.Transformed) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "  \"\"\"Computes loss.\"\"\"\n",
        "  pred_graph = net.apply(params, graph)\n",
        "  preds = decode(pred_graph, senders, receivers)\n",
        "  loss = compute_bce_with_logits_loss(preds, labels)\n",
        "  return loss, preds\n",
        "\n",
        "\n",
        "def compute_roc_auc_score(preds: jnp.ndarray,\n",
        "                          labels: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Computes roc auc (area under the curve) score for classification.\"\"\"\n",
        "  s = jax.nn.sigmoid(preds)\n",
        "  roc_auc = roc_auc_score(labels, s)\n",
        "  return roc_auc"
      ],
      "metadata": {
        "id": "8_YYSb098lDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from typing import Tuple\n",
        "import jax\n",
        "\n",
        "def negative_sampling(\n",
        "    graph: jraph.GraphsTuple, num_neg_samples: int,\n",
        "    key: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "  \"\"\"Samples negative edges, i.e. edges that don't exist in the input graph.\"\"\"\n",
        "  num_nodes = graph.n_node[0]\n",
        "  total_possible_edges = num_nodes**2\n",
        "  # convert 2D edge indices to 1D representation.\n",
        "  pos_idx = graph.senders * num_nodes + graph.receivers\n",
        "\n",
        "  # Percentage to oversample edges, so most likely will sample enough neg edges.\n",
        "  alpha = jnp.abs(1 / (1 - 1.1 *\n",
        "                       (graph.senders.shape[0] / total_possible_edges)))\n",
        "\n",
        "  perm = jax.random.randint(\n",
        "      key,\n",
        "      shape=(int(alpha * num_neg_samples),),\n",
        "      minval=0,\n",
        "      maxval=total_possible_edges,\n",
        "      dtype=jnp.uint32)\n",
        "\n",
        "  # mask where sampled edges are positive edges.\n",
        "  mask = jnp.isin(perm, pos_idx)\n",
        "  # remove positive edges.\n",
        "  perm = perm[~mask][:num_neg_samples]\n",
        "\n",
        "  # convert 1d back to 2d edge indices.\n",
        "  neg_senders = perm // num_nodes\n",
        "  neg_receivers = perm % num_nodes\n",
        "\n",
        "  return neg_senders, neg_receivers\n"
      ],
      "metadata": {
        "id": "vD9vRvEk8n4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset: List[Dict[str, Any]], num_epochs: int) -> hk.Params:\n",
        "  \"\"\"Training loop.\"\"\"\n",
        "  key = jax.random.PRNGKey(42)\n",
        "  # Transform impure `net_fn` to pure functions with hk.transform.\n",
        "  net = hk.without_apply_rng(hk.transform(net_fn))\n",
        "  # Get a candidate graph and label to initialize the network.\n",
        "  graph = dataset[0]['input_graph']\n",
        "\n",
        "  train_graph, _, val_pos_s, val_pos_r, val_neg_s, val_neg_r, test_pos_s, \\\n",
        "      test_pos_r, test_neg_s, test_neg_r = train_val_test_split_edges(\n",
        "      graph)\n",
        "\n",
        "  # Prepare the validation and test data.\n",
        "  val_senders = jnp.concatenate((val_pos_s, val_neg_s))\n",
        "  val_receivers = jnp.concatenate((val_pos_r, val_neg_r))\n",
        "  val_labels = jnp.concatenate(\n",
        "      (jnp.ones(len(val_pos_s)), jnp.zeros(len(val_neg_s))))\n",
        "  test_senders = jnp.concatenate((test_pos_s, test_neg_s))\n",
        "  test_receivers = jnp.concatenate((test_pos_r, test_neg_r))\n",
        "  test_labels = jnp.concatenate(\n",
        "      (jnp.ones(len(test_pos_s)), jnp.zeros(len(test_neg_s))))\n",
        "  # Initialize the network.\n",
        "  params = net.init(key, train_graph)\n",
        "  # Initialize the optimizer.\n",
        "  opt_init, opt_update = optax.adam(1e-4)\n",
        "  opt_state = opt_init(params)\n",
        "\n",
        "  compute_loss_fn = functools.partial(compute_loss, net=net)\n",
        "  # We jit the computation of our loss, since this is the main computation.\n",
        "  # Using jax.jit means that we will use a single accelerator. If you want\n",
        "  # to use more than 1 accelerator, use jax.pmap. More information can be\n",
        "  # found in the jax documentation.\n",
        "  compute_loss_fn = jax.jit(jax.value_and_grad(compute_loss_fn, has_aux=True))\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    num_neg_samples = train_graph.senders.shape[0]\n",
        "    train_neg_senders, train_neg_receivers = negative_sampling(\n",
        "        train_graph, num_neg_samples=num_neg_samples, key=key)\n",
        "    train_senders = jnp.concatenate((train_graph.senders, train_neg_senders))\n",
        "    train_receivers = jnp.concatenate(\n",
        "        (train_graph.receivers, train_neg_receivers))\n",
        "    train_labels = jnp.concatenate(\n",
        "        (jnp.ones(len(train_graph.senders)), jnp.zeros(len(train_neg_senders))))\n",
        "\n",
        "    (train_loss,\n",
        "     train_preds), grad = compute_loss_fn(params, train_graph, train_senders,\n",
        "                                          train_receivers, train_labels)\n",
        "\n",
        "    updates, opt_state = opt_update(grad, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    if epoch % 10 == 0 or epoch == (num_epochs - 1):\n",
        "      train_roc_auc = compute_roc_auc_score(train_preds, train_labels)\n",
        "      val_loss, val_preds = compute_loss(params, train_graph, val_senders,\n",
        "                                         val_receivers, val_labels, net)\n",
        "      val_roc_auc = compute_roc_auc_score(val_preds, val_labels)\n",
        "      print(f'epoch: {epoch}, train_loss: {train_loss:.3f}, '\n",
        "            f'train_roc_auc: {train_roc_auc:.3f}, val_loss: {val_loss:.3f}, '\n",
        "            f'val_roc_auc: {val_roc_auc:.3f}')\n",
        "  test_loss, test_preds = compute_loss(params, train_graph, test_senders,\n",
        "                                       test_receivers, test_labels, net)\n",
        "  test_roc_auc = compute_roc_auc_score(test_preds, test_labels)\n",
        "  print('Training finished')\n",
        "  print(\n",
        "      f'epoch: {epoch}, test_loss: {test_loss:.3f}, test_roc_auc: {test_roc_auc:.3f}'\n",
        "  )\n",
        "  return params"
      ],
      "metadata": {
        "id": "7lG2BD129g8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = train(cora_ds, num_epochs=200)"
      ],
      "metadata": {
        "id": "us5P9zRb9iZ_",
        "outputId": "b76ddceb-7af5-4491-b6bd-1d1f0036df0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, train_loss: 0.690, train_roc_auc: 0.601, val_loss: 0.690, val_roc_auc: 0.608\n",
            "epoch: 10, train_loss: 0.685, train_roc_auc: 0.702, val_loss: 0.687, val_roc_auc: 0.668\n",
            "epoch: 20, train_loss: 0.680, train_roc_auc: 0.761, val_loss: 0.683, val_roc_auc: 0.704\n",
            "epoch: 30, train_loss: 0.673, train_roc_auc: 0.791, val_loss: 0.678, val_roc_auc: 0.725\n",
            "epoch: 40, train_loss: 0.663, train_roc_auc: 0.812, val_loss: 0.672, val_roc_auc: 0.744\n",
            "epoch: 50, train_loss: 0.651, train_roc_auc: 0.833, val_loss: 0.663, val_roc_auc: 0.767\n",
            "epoch: 60, train_loss: 0.636, train_roc_auc: 0.853, val_loss: 0.651, val_roc_auc: 0.789\n",
            "epoch: 70, train_loss: 0.617, train_roc_auc: 0.870, val_loss: 0.637, val_roc_auc: 0.806\n",
            "epoch: 80, train_loss: 0.595, train_roc_auc: 0.882, val_loss: 0.620, val_roc_auc: 0.816\n",
            "epoch: 90, train_loss: 0.571, train_roc_auc: 0.890, val_loss: 0.601, val_roc_auc: 0.821\n",
            "epoch: 100, train_loss: 0.546, train_roc_auc: 0.895, val_loss: 0.584, val_roc_auc: 0.826\n",
            "epoch: 110, train_loss: 0.523, train_roc_auc: 0.901, val_loss: 0.569, val_roc_auc: 0.832\n",
            "epoch: 120, train_loss: 0.502, train_roc_auc: 0.908, val_loss: 0.558, val_roc_auc: 0.838\n",
            "epoch: 130, train_loss: 0.482, train_roc_auc: 0.918, val_loss: 0.549, val_roc_auc: 0.845\n",
            "epoch: 140, train_loss: 0.464, train_roc_auc: 0.927, val_loss: 0.542, val_roc_auc: 0.852\n",
            "epoch: 150, train_loss: 0.448, train_roc_auc: 0.936, val_loss: 0.537, val_roc_auc: 0.857\n",
            "epoch: 160, train_loss: 0.432, train_roc_auc: 0.943, val_loss: 0.533, val_roc_auc: 0.861\n",
            "epoch: 170, train_loss: 0.417, train_roc_auc: 0.950, val_loss: 0.530, val_roc_auc: 0.864\n",
            "epoch: 180, train_loss: 0.402, train_roc_auc: 0.956, val_loss: 0.528, val_roc_auc: 0.866\n",
            "epoch: 190, train_loss: 0.388, train_roc_auc: 0.962, val_loss: 0.527, val_roc_auc: 0.868\n",
            "epoch: 199, train_loss: 0.375, train_roc_auc: 0.966, val_loss: 0.528, val_roc_auc: 0.869\n",
            "Training finished\n",
            "epoch: 199, test_loss: 0.554, test_roc_auc: 0.851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBNNJtF33DOJ"
      },
      "source": [
        "The test performance shows that we obtain quite good scores on an unseen part of the dataset. It should be noted that as we have been using the test set for validation as well, we might have overfitted slightly to this set. Nevertheless, the experiment shows us that GNNs can be indeed powerful to predict the properties of graphs and/or molecules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ifbmgt73DOJ"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we have seen the application of neural networks to graph structures. We looked at how a graph can be represented (adjacency matrix or edge list), and discussed the implementation of common graph layers: GCN and GAT. The implementations showed the practical side of the layers, which is often easier than the theory. Finally, we experimented with different tasks, on node-, edge- and graph-level. Overall, we have seen that including graph information in the predictions can be crucial for achieving high performance. There are a lot of applications that benefit from GNNs, and the importance of these networks will likely increase over the next years."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSISxQyR3DON"
      },
      "source": [
        "---\n",
        "\n",
        "[![Star our repository](https://img.shields.io/static/v1.svg?logo=star&label=⭐&message=Star%20Our%20Repository&color=yellow)](https://github.com/phlippe/uvadlc_notebooks/)  If you found this tutorial helpful, consider ⭐-ing our repository.    \n",
        "[![Ask questions](https://img.shields.io/static/v1.svg?logo=star&label=❔&message=Ask%20Questions&color=9cf)](https://github.com/phlippe/uvadlc_notebooks/issues)  For any questions, typos, or bugs that you found, please raise an issue on GitHub.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}